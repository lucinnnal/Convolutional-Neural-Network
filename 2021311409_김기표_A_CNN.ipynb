{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucinnnal/Convolutional-Neural-Network/blob/main/2021311409_%EA%B9%80%EA%B8%B0%ED%91%9C_A_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LTTarYu4cvmu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**axis=(a, b)는 지정한 두 축 a와 b를 따라 값을 합산하고, 나머지 축만 남김.\n",
        "다차원 배열의 특정 부분을 처리할 때 유용**"
      ],
      "metadata": {
        "id": "ItKSaqGGhlo6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YC7ITiBIcvmx"
      },
      "outputs": [],
      "source": [
        "class Conv3x3:\n",
        "\n",
        "    def __init__(self, num_filters):\n",
        "\n",
        "        self.num_filters = num_filters #filter 의 개수 == output의 차원\n",
        "        self.filters = np.random.randn(num_filters, 3, 3) / 9 # initialize num_filters x 3 x 3\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        self.last_input = input\n",
        "\n",
        "        h, w = input.shape\n",
        "        output = np.zeros((h-2, w-2, self.num_filters)) # write code here (1 line) -> assuming strde 1?\n",
        "\n",
        "######write code here (3 lines)######\n",
        "#\n",
        "#        for i in range(None):\n",
        "#            for j in range(None):\n",
        "#\n",
        "#\n",
        "#####################################\n",
        "        for i in range(output.shape[0]):\n",
        "          for j in range(output.shape[1]):\n",
        "            output[i,j,:] = np.sum(input[i:i+3, j:j+3] * self.filters, axis = (1,2))\n",
        "\n",
        "        return output\n",
        "\n",
        "class MaxPool2: #2x2 maxpooling\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        self.last_input = input\n",
        "        h, w, num_filters = input.shape\n",
        "\n",
        "        output = np.zeros((h // 2, w // 2, num_filters)) #write code here (1 line)\n",
        "\n",
        "######write code here (3 lines)######\n",
        "#\n",
        "#        for i in range(None):\n",
        "#            for j in range(None):\n",
        "#\n",
        "#####################################\n",
        "        for i in range(output.shape[0]):\n",
        "          for j in range(output.shape[1]):\n",
        "            output[i,j,:] = np.max(np.transpose(input[2*i:2*(i+1),2*j:2*(j+1),:], (2,0,1)), axis = (1,2))\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self, input_len, nodes):\n",
        "\n",
        "        self.weights = np.random.randn(input_len, nodes) / input_len\n",
        "        self.biases = np.zeros(nodes)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.last_input_shape = input.shape # Caching for backprop\n",
        "\n",
        "        input = input.flatten() # input shape = (input_len,)\n",
        "        self.last_input = input # Caching for backprop\n",
        "\n",
        "        totals = np.dot(self.weights.T, input) + self.biases # write code here (1 line) -> totals shape : (10,)\n",
        "\n",
        "        self.last_totals = totals # Caching for backprop\n",
        "\n",
        "        exp_a = np.exp(totals) # write code here (1 line)\n",
        "        return exp_a / np.sum(exp_a) # write code here (1 line) -> (10,)\n",
        "\n",
        "\n",
        "    def backprop(self, dL_dOut, learn_rate):\n",
        "        for i, gradient in enumerate(dL_dOut):\n",
        "            if gradient == 0:\n",
        "                continue # 정답 index에 대해서만 gradient 계산을 할 것임.\n",
        "\n",
        "            # e^totals\n",
        "            t_exp = np.exp(self.last_totals) # write code here (1 line)\n",
        "\n",
        "            # Sum of all e^totals\n",
        "            S = np.sum(t_exp) # write code here (1 line)\n",
        "\n",
        "            # Gradient of out[i] against totals\n",
        "            dOut_dt =  -1 * (t_exp / S) * (t_exp / S)[i]  # write code here (1 line) -> (10,1)\n",
        "            dOut_dt[i] =  (t_exp / S)[i] * (1 - (t_exp / S)[i]) # write code here (1 line) (y_hat - y)\n",
        "\n",
        "            # weights/biases/input gradient #\n",
        "            # write code here (3 lines)\n",
        "            dt_dw = self.last_input\n",
        "            dt_db = np.ones(10)\n",
        "            dt_dInputs = self.weights # v\n",
        "            ################################\n",
        "\n",
        "            # compute gradient\n",
        "            # write code here (4 lines)\n",
        "            dL_dt = dL_dOut[i] * dOut_dt # ->(10,)\n",
        "            dL_dw = np.dot(dt_dw[:, np.newaxis], dL_dt[:, np.newaxis].T)\n",
        "            dL_db = dL_dt * dt_db # (10,)\n",
        "            dL_dInputs = np.dot(dt_dInputs, dL_dt[:,np.newaxis]) # (input_len, 1)\n",
        "\n",
        "            # update weights/biases\n",
        "            # write code here (2 lines)\n",
        "            self.weights -= learn_rate * dL_dw\n",
        "            self.biases -= learn_rate * dL_db\n",
        "            return dL_dInputs.reshape(self.last_input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRu99nhrcvmy"
      },
      "source": [
        "# Test (수정 X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Hmtrsgx_cvmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b4e1bf3-261c-46c4-a4fa-a75574187dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 100] Past 100 steps: Average Loss 2.217 | Accuracy: 20%\n",
            "[Step 200] Past 100 steps: Average Loss 2.101 | Accuracy: 35%\n",
            "[Step 300] Past 100 steps: Average Loss 1.944 | Accuracy: 51%\n",
            "[Step 400] Past 100 steps: Average Loss 1.794 | Accuracy: 69%\n",
            "[Step 500] Past 100 steps: Average Loss 1.711 | Accuracy: 60%\n",
            "[Step 600] Past 100 steps: Average Loss 1.741 | Accuracy: 54%\n",
            "[Step 700] Past 100 steps: Average Loss 1.656 | Accuracy: 64%\n",
            "[Step 800] Past 100 steps: Average Loss 1.526 | Accuracy: 70%\n",
            "[Step 900] Past 100 steps: Average Loss 1.441 | Accuracy: 76%\n",
            "[Step 1000] Past 100 steps: Average Loss 1.414 | Accuracy: 75%\n"
          ]
        }
      ],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images[:1000]\n",
        "train_labels = train_labels[:1000]\n",
        "\n",
        "conv = Conv3x3(8)\n",
        "pool = MaxPool2()\n",
        "softmax = Softmax(13 * 13 * 8, 10)\n",
        "\n",
        "def forward(image, label):\n",
        "    out = conv.forward((image / 255) - 0.5)\n",
        "    out = pool.forward(out)\n",
        "    out = softmax.forward(out)\n",
        "\n",
        "    loss = -np.log(out[label]) # Cross Entropy\n",
        "    acc = 1 if np.argmax(out) == label else 0 # 하나 하나씩 학습시키는 것임\n",
        "\n",
        "    return out, loss, acc\n",
        "\n",
        "def train(im, label, lr = .005):\n",
        "    # Forward\n",
        "    out, loss, acc = forward(im, label)\n",
        "\n",
        "    # Calculate initial gradient\n",
        "    gradient = np.zeros(10)\n",
        "    gradient[label] = -1/out[label]\n",
        "\n",
        "    # Backprop\n",
        "    gradient = softmax.backprop(gradient, lr)\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "loss = 0\n",
        "num_correct = 0\n",
        "for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
        "    if i % 100 == 99:\n",
        "        print(\n",
        "            '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
        "            (i + 1, loss / 100, num_correct)\n",
        "        )\n",
        "        loss = 0\n",
        "        num_correct = 0\n",
        "\n",
        "    l, acc = train(im, label)\n",
        "    loss += l\n",
        "    num_correct += acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFjeOhVKcvmz"
      },
      "source": [
        "# 수고하셨습니다 :)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}